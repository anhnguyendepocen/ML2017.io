---
title: "Lab 5  -- Subset Selection"
author: "Philipp Broniecki and Lucas Leemann -- Machine Learning 1K"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(global.par = TRUE)
```

##### (based on James et al. 2013, chapter 6)

We start by clearing our workspace.

```{r}
# clear workspace
rm( list = ls() )
```

### Subset Selection Methods

We use a modified data set on non-western immigrants (we inserted some missings). Download the data [here](http://philippbroniecki.github.io/ML2017.io/data/BSAS_manip_missings.RData).

The codebook is:

|Variable Name|Description|
|--------|-----------------------------------------------------------|
|IMMBRIT | Out of every 100 people in Britain, how many do you think are immigrants from Non-western countries?|
|over.estimate | 1 if estimate is higher than 10.7%. |
|RSex | 1 = male, 2 = female |
|RAge | Age of respondent |
|Househld | Number of people living in respondent's household |
|Cons, Lab, SNP, Ukip, BNP, GP, party.other | Party self-identification|
|paper | Do you normally read any daily morning newspaper 3+ times/week? |
|WWWhourspW | How many hours WWW per week? |
|religious | Do you regard yourself as belonging to any particular religion? |
|employMonths | How many mnths w. present employer? |
|urban | Population density, 4 categories (highest density is 4, lowest is 1) |
|health.good | How is your health in general for someone of your age? (0: bad, 1: fair, 2: fairly good, 3: good) |
|HHInc | Income bands for household, high number = high HH income |

```{r, eval=FALSE}
# load foreigners data
load("your directory/BSAS_manip_missings.RData")
```

```{r, include=FALSE}
# load foreigners data
load("./data/BSAS_manip_missings.RData")
df <- data2
rm(data2)
```

We check our data set for missing values variable by variable using `apply()`, `is.na()`, and `table()`.

```{r}
# check for missing values
apply(df, 2, function(x) table(is.na(x))["TRUE"] )

# we drop missings in IMMBRIT
df <- df[ !is.na(df$IMMBRIT), ]

# to drop variables on the entire dataset (uncomment next line)
#df <- na.omit(df)
```

We now declare the categorical variables to be factors and create a copy of the main data set that excludes `over.estimate`.

```{r}
# declare factor variables
df$urban <- factor(df$urban, labels = c("rural", "more rural", "more urban", "urban"))
df$RSex <- factor(df$RSex, labels = c("Male", "Female"))
df$health.good <- factor(df$health.good, labels = c("bad", "fair", "fairly good", "good") )

# drop the binary response coded 1 if IMMBRIT > 10.7 
df$over.estimate <- NULL
df$Cons <- NULL
```


### Best Subset Selection

We use the `regsubsets()` function to identify the best model based on subset selection quantified by the residual sum of squares (RSS) for each model.

```{r}
library(leaps)

# run best subset selection
regfit.full <- regsubsets(IMMBRIT ~ ., data = df)
summary(regfit.full)
```

With the `nvmax` parameter we control the number of variables in the model. The default used by `regsubsets()` is 8.

```{r}
# increase the max number of variables
regfit.full <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 16)
reg.summary <- summary(regfit.full)
```

We can look at the components of the `reg.summary` object using the `names()` function and examine the $R^2$ statistic stored in `rsq`.

```{r}
names(reg.summary)
reg.summary$rsq
```

Next, we plot the $RSS$ and adjusted $R^2$ and add a point where $R^2$ is at its maximum using the `which.max()` function.

```{r, eval=FALSE}
par( mfrow =  c(2,2) ) # 2 row, 2 columns in plot window
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "Residual Sum of Squares", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", type = "l")

# find the peak of adj. R^2
adjr2.max <- which.max( reg.summary$adjr2 )
points(adjr2.max, reg.summary$adjr2[adjr2.max], col = "red", pch = 20, cex = 2)
```

```{r, echo=FALSE, fig.height=3}
par( mfrow =  c(1,2) ) # 2 row, 2 columns in plot window
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "Residual Sum of Squares", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", type = "l")

# find the peak of adj. R^2
adjr2.max <- which.max( reg.summary$adjr2 )
points(adjr2.max, reg.summary$adjr2[adjr2.max], col = "red", pch = 20, cex = 2)
```

We can also plot the $C_{p}$ statistic and $BIC$ and identify the minimum points for each statistic using the $which.min()$ function.

```{r, eval=FALSE}
# cp
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp.min <- which.min(reg.summary$cp)
points(cp.min, reg.summary$cp[cp.min], col = "red", cex = 2, pch = 20)

# bic
bic.min <- which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(bic.min, reg.summary$bic[bic.min], col = "red", cex = 2, pch = 20)
```

```{r, echo=FALSE}
par( mfrow =  c(2,2) ) # 2 row, 2 columns in plot window
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "Residual Sum of Squares", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", type = "l")

# find the peak of adj. R^2
adjr2.max <- which.max( reg.summary$adjr2 )
points(adjr2.max, reg.summary$adjr2[adjr2.max], col = "red", pch = 20, cex = 2)

# cp
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp.min <- which.min(reg.summary$cp)
points(cp.min, reg.summary$cp[cp.min], col = "red", cex = 2, pch = 20)

# bic
bic.min <- which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(bic.min, reg.summary$bic[bic.min], col = "red", cex = 2, pch = 20)
```

The estimated models from `regsubsets()` can be directly plotted to compare the differences based on the values of $R^2$, adjusted $R^2$, $C_{p}$ and $BIC$ statistics.

```{r, fig.height = 6}
par( mfrow = c(1,1), oma = c(3,0,0,0))

# plot model comparison based on R^2
plot(regfit.full, scale = "r2")

# plot model comparison based on adjusted R^2
plot(regfit.full, scale = "adjr2")


# plot model comparison based on adjusted CP
plot(regfit.full, scale = "Cp")


# plot model comparison based on adjusted BIC
plot(regfit.full, scale = "bic")
```

To show the coefficients associated with the model with the lowest $BIC$, we use the `coef()` function.

```{r}
coef(regfit.full, bic.min)
```



### Forward and Backward Stepwise Selection

The default method used by `regsubsets()` is exhaustive but we can change it to forward or backward and compare the results.

```{r}
# run forward selection
regfit.fwd <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 16, method = "forward")
summary(regfit.fwd)

# run backward selection
regfit.bwd <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 16, method = "backward")
summary(regfit.bwd)

# mdoel coefficients of best 7-variable models
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```



#### Choosing Among Models Using the Validation Set Approach and Cross-Validation

For validation set approach, we split the dataset into a training subset and a test subset. In order to ensure that the results are consistent over multiple iterations, we set the random seed with `set.seed()` before calling `sample()`.

```{r}
set.seed(1)

# sample true or false for each observation
train <- sample( c(TRUE, FALSE), size = nrow(df), replace = TRUE )
# the complement
test <- (!train)
```

We use `regsubsets()` as we did in the last section, but limit the estimation to the training subset.

```{r}
regfit.best <- regsubsets(IMMBRIT ~ ., data = df[train, ], nvmax = 16)
```

We create a matrix from the test subset using `model.matrix()`. Model matrix takes the dependent variable out of the data and adds an intercept to it.

```{r}
# test data
test.mat <- model.matrix(IMMBRIT ~., data = df[test, ])
```

Next, we compute the validation error for each model.

```{r}
# validation error for each model
val.errors <- NA
for (i in 1:16 ){
  coefi <- coef(regfit.best, id = i)
  y_hat <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean(  (df$IMMBRIT[test] - y_hat)^2   )
}
```

We examine the validation error for each model and identify the best model with the lowest error.

```{r}
val.errors

# which model has smallest error
min.val.errors <- which.min(val.errors)
# coefficients of that model
coef( regfit.best, min.val.errors )
```

We can combine these steps into a function that can be called repeatedly when running k-fold cross-validation.

```{r}
# precict function for repeatedly choosing model with lowest test error
predict.regsubsets <- function( object, newdata, id, ... ){
  # get the formula from the model
  m.formula <- as.formula( object$call[[2]] )
  # use that formula to create the model matrix for some new data
  mat <- model.matrix( m.formula, newdata )
  # get coeffiecients where id is the number of variables
  coefi <- coef( object, id = id )
  # get the variable names of current model
  xvars <- names( coefi )
  # multiply data with coefficients
  mat[ , xvars ] %*% coefi
}
```

We run `regsubsets()` on the full dataset and examine the coefficients associated with the model that has the lower validation error.

```{r}
# best subset on full data set
regfit.best <- regsubsets( IMMBRIT ~ ., data = df, nvmax = 16 )

# examine coefficients of the model that had the lowest validation error
coef( regfit.best, min.val.errors )
```

#### k-fold cross-validation

For cross-validation, we create the number of folds needed (10, in this case) and allocate a matrix for storing the results.

```{r}
# number of folds
k <- 10
set.seed(1)
# fold assignment for each observation
folds <- sample(1:k, nrow(df), replace = TRUE)
# frequency table of fold assignment (should be relatively balanced)
table(folds)
# container for cross-validation errors
cv.errors <- matrix(NA, nrow = k, ncol = 16, dimnames = list(NULL, paste(1:16)))
# have a look at the matrix
cv.errors
```

We then run through each fold in a `for()` loop and predict the salary using our predict function. We then calculate the validation error for each fold and save them in the matrix created above.

```{r}
# loop over folds
for (a in 1:k){
  
  # best subset selection on training data
  best.fit <- regsubsets(IMMBRIT ~ ., data = df[ folds != a, ], nvmax = 16)
  
  # loop over the 16 subsets
  for (b in 1:16){
    
    # predict response for test set for current subset
    pred <- predict(best.fit, df[ folds == a ,], id = b )
    
    # MSE into container; rows are folds; columns are subsets
    cv.errors[a, b] <- mean( (df$IMMBRIT[folds==a] - pred)^2 )
    
  } # end of loop over the 16 subsets
} # end of loop over folds
# the cross-validation error matrix
cv.errors
```

We calculate the mean error for all subsets by applying mean to each column using the `apply()` function.

```{r}
# average cross-validation errors over the folds
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

# visualize
par( mfrow = c(1,1) , oma = c(0,0,0,0))
plot( mean.cv.errors, type = "b" )
```

Finally, we run `regsubsets()` on the full dataset and show the coefficients for the best performing model which we picked using 10-fold cross-validation.

```{r}
# run regsubsets on full data set
reg.best <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 16)
# coefficients of subset which minimized test error
coef(reg.best, which.min(mean.cv.errors))
```

### Exercises

#### Q1

In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

1. Use the `rnorm()` function to generate a predictor $X$ of length $n=100$, as well as a noise vector $\epsilon$ of length $n=100$.
2. Generate a response vector $Y$ of length $n=100$ according to the model $$ Y = \beta_{0} + \beta_{1} X + \beta_{2} X^{2} + \beta_{3} X^{3} + \epsilon,$$
where $\beta_{0},\: \beta_{1}, \: \beta_{2}, \: \beta_{3}$ are constants of your choice.
3. Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2,\ldots,X^{10}$. What is the best model obtained according to $C_{p}$, $BIC$, and adjusted $R^2$ Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the `data.frame()` function to create a single data set containing both $X$ and $Y$.
4. Repeat (3), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (3)?

#### Q2

We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.

1. Generate a data set with $p=20$ features, $n=1,000$ observations, and an associated quantitative response vector generated according to the model $$Y = X\beta+\epsilon,$$ where $\beta$ has some elements that are exactly equal to $0$.
2. Split your dataset into a training set containing 100 observations and a test set containing 900 observations.
3. Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.
4. Plot the test set MSE associated with the best model of each size.
5. For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in 1. until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.