---
title: "Lab 5  -- Subset Selection"
author: "Philipp Broniecki and Lucas Leemann -- Machine Learning 1K"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(global.par = TRUE)
```

##### (based on James et al. 2013, chapter 6)

We start by clearing our workspace.

```{r}
# clear workspace
rm( list = ls() )
```

### Subset Selection Methods

We use a modified data set on non-western immigrants (we inserted some missings).The codebook is:

Let's check the codebook of our data.

|Variable Name|Description|
|--------|-----------------------------------------------------------|
|IMMBRIT | Out of every 100 people in Britain, how many do you think are immigrants from Non-western countries?|
|over.estimate | 1 if estimate is higher than 10.7%. |
|RSex | 1 = male, 2 = female |
|RAge | Age of respondent |
|Househld | Number of people living in respondent's household |
|Cons, Lab, SNP, Ukip, BNP, GP, party.other | Party self-identification|
|paper | Do you normally read any daily morning newspaper 3+ times/week? |
|WWWhourspW | How many hours WWW per week? |
|religious | Do you regard yourself as belonging to any particular religion? |
|employMonths | How many mnths w. present employer? |
|urban | Population density, 4 categories (highest density is 4, lowest is 1) |
|health.good | How is your health in general for someone of your age? (0: bad, 1: fair, 2: fairly good, 3: good) |
|HHInc | Income bands for household, high number = high HH income |

```{r, eval=FALSE}
# load foreigners data
load("your directory/BSAS_manip_missings.RData")
```

```{r, include=FALSE}
# load foreigners data
load("./data/BSAS_manip_missings.RData")
```

We check our data set for missing values variable by variable using `apply()`, `is.na()`, and `table()`.

```{r}
# check for missing values
apply(data2, 2, function(x) table(is.na(x))["TRUE"] )

#  immbrit has missings which we drop
data2 <- data2[ !is.na(data2$IMMBRIT), ]
```

We now declare the categorical variables to be factors and create a copy of the main data set that excludes `over.estimate`.

```{r}
# declare factor variables
data2$urban <- factor(data2$urban, labels = c("rural", "more rural", "more urban", "urban"))
data2$RSex <- factor(data2$RSex, labels = c("Male", "Female"))
data2$health.good <- factor(data2$health.good, labels = c("bad", "fair", "fairly good", "good") )

# keep only variables to use for subset selection 
df <- dplyr::select(data2, IMMBRIT, RSex, RAge, Househld, Lab, SNP, Ukip,
                    BNP, GP, party.other, paper, WWWhourspW, religious, employMonths,
                    urban, health.good, HHInc)
```



### Best Subset Selection

We use the `regsubsets()` function to identify the best model based on subset selection quantified by the residual sum of squares (RSS) for each model.

```{r}
library(leaps)

# run best subset selection
regfit.full <- regsubsets(IMMBRIT ~ ., data = df)
summary(regfit.full)
```

The `nvmax` parameter can be use to control the number of variables in the model. The default used by `regsubsets()` is 8.

```{r}
# increase the max number of variables
regfit.full <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 20)
reg.summary <- summary(regfit.full)
```

We can look at the components of the `reg.summary` object using the `names()` function and examine the $R^2$ statistic stored in `rsq`.

```{r}
names(reg.summary)
reg.summary$rsq
```

Next, we plot the $RSS$ and adjusted $R^2$ and add a point where $R^2$ is at its maximum using the `which.max()` function.

```{r, more.plotting}
par( mfrow =  c(2,2) ) # 2 row, 2 columns in plot window
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "Residual Sum of Squares", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", type = "l")

# find the peak of adj. R^2
adjr2.max <- which.max( reg.summary$adjr2 )
points(adjr2.max, reg.summary$adjr2[adjr2.max], col = "red", pch = 20, cex = 2)
```

We can also plot the the $C_{p}$ statistic and $BIC$ and identify the minimum points for each statistic using the $which.min()$ function.

```{r}
<<more.plotting>>
# cp
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp.min <- which.min(reg.summary$cp)
points(cp.min, reg.summary$cp[cp.min], col = "red", cex = 2, pch = 20)

# bic
bic.min <- which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(bic.min, reg.summary$bic[bic.min], col = "red", cex = 2, pch = 20)
```

The estimated models from `regsubsets()` can be directly plotted to compare the differences based on the values of $R^2$, adjusted $R^2$, $C_{p}$ and $BIC$ statistics.

```{r, fig.height = 6}
par( mfrow = c(1,1), oma = c(3,0,0,0))

# plot model comparison based on R^2
plot(regfit.full, scale = "r2")

# plot model comparison based on adjusted R^2
plot(regfit.full, scale = "adjr2")


# plot model comparison based on adjusted CP
plot(regfit.full, scale = "Cp")


# plot model comparison based on adjusted BIC
plot(regfit.full, scale = "bic")
```

To show the coefficients associated with the model with the lowest $BIC$, we use the `coef()` function.

```{r}
coef(regfit.full, bic.min)
```



### Forward and Backward Stepwise Selection

The default method used by `regsubsets()` is exhaustive but we can change it to forward or backward and compare the results.

```{r}
# run forward selection
regfit.fwd <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 20, method = "forward")
summary(regfit.fwd)

# run backward selection
regfit.bwd <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 20, method = "backward")
summary(regfit.bwd)

# mdoel coefficients of best 7-variable models
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```



#### Choosing Among Models Using the Validation Set Approach and Cross-Validation

For validation set approach, we split the dataset into a training subset and a test subset. In order to ensure that the results are consistent over multiple iterations, we set the random seed with `set.seed()` before calling `sample()`.

```{r}
set.seed(1)

# sample true or false for each observation
train <- sample( c(TRUE, FALSE), size = nrow(df), replace = TRUE )
# the complement
test <- (!train)
```

We use `regsubsets()` as we did in the last section, but limit the estimation to the training subset.

```{r}
regfit.best <- regsubsets(IMMBRIT ~ ., data = df[train, ], nvmax = 20)
```

We create a matrix from the test subset using `model.matrix()`.

```{r}
# test data
test.mat <- model.matrix(IMMBRIT ~., data = df[test, ])
```

Next, we compute the validation error for each model.

```{r}
# validation error for each model
val.errors <- NA
for (i in 1:20 ){
  coefi <- coef(regfit.best, id = i)
  y_hat <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean(  (df$IMMBRIT[test] - y_hat)^2   )
}
```

We examine the validation error for each model and identify the best model with the lowest error.

```{r}
val.errors

# which model has smallest error
min.val.errors <- which.min(val.errors)
# coefficients of that model
coef( regfit.best, min.val.errors )
```

We can combine these steps into a function that can be called repeatedly when running k-fold cross-validation.

```{r}
# precict function for repeatedly choosing model with lowest test error
predict.regsubsets <- function( object, newdata, id, ... ){
  m.formula <- as.formula( object$call[[2]] )
  mat <- model.matrix( m.formula, newdata )
  coefi <- coef( object, id = id )
  xvars <- names( coefi )
  mat[ , xvars ] %*% coefi
}
```

As a final step, we run `regsubsets()` on the full dataset and examine the coefficients associated with the model that has the lower validation error.

```{r}
# best subset on full data set
regfit.best <- regsubsets( IMMBRIT ~ ., data = df, nvmax = 20 )

# examine coefficients of the model that had the lowest validation error
coef( regfit.best, min.val.errors )
```

For cross-validation, we create the number of folds needed (10, in this case) and allocate a matrix for storing the results.

```{r}
# number of folds
k <- 10
set.seed(1)
# fold assignment for each observation
folds <- sample(1:k, nrow(df), replace = TRUE)
# container for cross-validation errors
cv.errors <- matrix(NA, nrow = k, ncol = 20, dimnames = list(NULL, paste(1:20)))
```

We then run through each fold in a `for()` loop and predict the salary using our predict function. We then calculate the validation error for each fold and save them in the matrix created above.

```{r}
# loop over folds
for (a in 1:k){
  
  # best subset selection on training data
  best.fit <- regsubsets(IMMBRIT ~ ., data = df[ folds != a, ], nvmax = 20)
  
  # loop over the 20 subsets
  for (b in 1:20){
    
    # predict response for test set for current subset
    pred <- predict(best.fit, df[ folds == a ,], id = b )
    
    # MSE into container; rows are folds; columns are subsets
    cv.errors[a, b] <- mean( (df$IMMBRIT[folds==a] - pred)^2 )
    
  } # end of loop over the 20 subsets
} # end of loop over folds
```

We calculate the mean error for all subsets by applying mean to each column using the `apply()` function.

```{r}
# average cross-validation errors over the folds
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

# visualize
par( mfrow = c(1,1) , oma = c(0,0,0,0))
plot( mean.cv.errors, type = "b" )
```

Finally, we run `regsubsets()` on the full dataset and show the coefficients for the best performing model.

```{r}
# run regsubsets on full data set
reg.best <- regsubsets(IMMBRIT ~ ., data = df, nvmax = 20)
# coefficients of subset which minimized test error
coef(reg.best, which.min(mean.cv.errors))
```

### Exercises

#### Q1

In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

1. Use the `rnorm()` function to generate a predictor $X$ of length $n=100$, as well as a noise vector &\epsilon& of length $n=100$.
1. Generate a response vector $Y$ of length $n=100$ according to the model $$ Y = \beta_{0} + \beta_{1} X + \beta_{2} X^{2} + \beta_{3} X^{3} + \epsilon,$$
where $\beta_{0},\: \beta_{1}, \: \beta_{2}, \: \beta_{3}$ are constants of your choice.
1. Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2,\ldots,X^{10}$. What is the best model obtained according to $C_{p}$, $BIC$, and adjusted $R^2$ Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the `data.frame()` function to create a single data set containing both $X$ and $Y$.
1. Repeat (3), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (3)?

#### Q2

We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.

1. Generate a data set with $p=20$ features, $n=1,000$ observations, and an associated quantitative response vector generated according to the model $$Y = X\beta+\epsilon,$$ where $\beta$ has some elements that are exactly equal to $0$.