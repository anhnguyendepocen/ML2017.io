<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Philipp Broniecki and Lucas Leemann – Machine Learning 1K" />


<title>Lab 6 – Regularization</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/sandstone.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Essex 2017 Machine Learning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 1
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day1.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D1%20-%20Intro%20ML.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions1.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 2
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day2.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D2%20-%20Intro%20ML.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions2.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 3
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day3.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D3%20-%20Classification.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions3.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 4
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day4.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D4%20-%20Resampling.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions4.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 5
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day5.html">Lab</a>
    </li>
    <li>
      <a href="labs/Lab%20Code%205.R">plain R-Code</a>
    </li>
    <li>
      <a href="./slides/D5%20-%20Model%20Selection%20I.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions5.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 6
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day6.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D6%20-%20Model%20Selection%20II.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions6.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 7
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day7.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D7%20-%20Polynomial%20Models.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions7.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 8
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day8.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D8%20-%20Tree-Based%20Methods.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions8.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 9
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day9.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D9%20-%20Unsupervised%20Learning.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions9.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="simulation.html">Simulation</a>
    </li>
    <li>
      <a href="montecarlo.html">Monte Carlos</a>
    </li>
    <li>
      <a href="MCLassoRidge.html">MC Lasso v. Ridge</a>
    </li>
    <li>
      <a href="splinesCV.html">Splines Cross Validated</a>
    </li>
    <li>
      <a href="./data/titanic.dta">Titanic Data</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Lab 6 – Regularization</h1>
<h4 class="author"><em>Philipp Broniecki and Lucas Leemann – Machine Learning 1K</em></h4>

</div>


<div id="based-on-james-et-al.-2013-chapter-6" class="section level5">
<h5>(based on James et al. 2013, chapter 6)</h5>
</div>
<div id="ridge-regression-and-the-lasso" class="section level3">
<h3>Ridge Regression and the Lasso</h3>
<p>We start by clearing our workspace, loading the foreigners data, and doing the necessary variable manipulations. The data is available <a href="http://philippbroniecki.github.io/ML2017.io/data/BSAS_manip.RData">here</a>.</p>
<pre class="r"><code># clear workspace
rm(list=ls())

# load foreigners data
load(&quot;your directory/BSAS_manip.RData&quot;)
head(data2)

# we declare the factor variables
data2$urban &lt;- factor(data2$urban, labels = c(&quot;rural&quot;, &quot;more rural&quot;, &quot;more urban&quot;, &quot;urban&quot;))
data2$RSex &lt;- factor(data2$RSex, labels = c(&quot;Male&quot;, &quot;Female&quot;))
data2$health.good &lt;- factor(data2$health.good, labels = c(&quot;bad&quot;, &quot;fair&quot;, &quot;fairly good&quot;, &quot;good&quot;) )

# categorical variables
cat.vars &lt;- unlist(lapply(data2, function(x) is.factor(x) | all(x == 0 | x==1) | all( x==1 | x==2) ))
# normalize numeric variables
data2[, !cat.vars] &lt;- apply(data2[, !cat.vars], 2, scale)</code></pre>
<p>In order to run ridge regression, we create a matrix from our dataset using the <code>model.matrix()</code> function. We also need to remove the intercept from the resulting matrix because the function to run ridge regression automatically includes one. Furthermore, we will use the subjective rate of immigrants as response. Consequently, we have to remove <code>over.estimate</code> as it measures the same thing. Lastly, the party affiliation dummies are mutually exclusive, se we have to exclude the model category <code>Cons</code>.</p>
<pre class="r"><code># covariates in matrix form but remove the intercept, over.estimate, and Cons
x &lt;- model.matrix(IMMBRIT ~ . -1 -over.estimate -Cons, data2)
# check if it looks fine
head(x)</code></pre>
<pre><code>##   RSexMale RSexFemale       RAge   Househld Lab SNP Ukip BNP GP
## 1        1          0  0.0144845 -0.2925308   1   0    0   0  0
## 2        0          1 -1.8065476  0.4540989   0   0    0   0  0
## 3        0          1  0.5835570 -1.0391604   0   0    0   0  0
## 4        0          1  1.5509804 -0.2925308   0   0    0   0  0
## 5        0          1  0.9819078 -1.0391604   0   0    0   0  0
## 6        1          0 -1.1236606  1.2007285   0   0    0   0  0
##   party.other paper WWWhourspW religious employMonths urbanmore rural
## 1           0     0 -0.5324636         0    -0.203378               0
## 2           1     0 -0.1566702         0    -0.203378               0
## 3           1     0 -0.5324636         0     5.158836               0
## 4           1     1 -0.4071991         1    -0.203378               0
## 5           1     0 -0.5324636         1    -0.203378               0
## 6           1     1  1.0959747         0    -0.203378               0
##   urbanmore urban urbanurban health.goodfair health.goodfairly good
## 1               0          1               1                      0
## 2               0          1               0                      1
## 3               1          0               0                      0
## 4               0          0               0                      0
## 5               1          0               0                      0
## 6               0          0               0                      1
##   health.goodgood      HHInc
## 1               0  0.7357918
## 2               0 -1.4195993
## 3               1 -0.1263647
## 4               1 -0.3419038
## 5               1 -0.1263647
## 6               0 -0.1263647</code></pre>
<pre class="r"><code># response vector
y &lt;- data2$IMMBRIT</code></pre>
<div id="ridge-regression" class="section level4">
<h4>Ridge Regression</h4>
<p>The <code>glmnet</code> package provides functionality to fit ridge regression and lasso models. We load the package and call <code>glmnet()</code> to perform ridge regression.</p>
<pre class="r"><code>library(glmnet)

# tuning parameter
grid &lt;- 10^seq(4, -2, length = 100)
plot(grid, bty = &quot;n&quot;, pch = 19,
     main = expression(paste(&quot;Grid of Tuning Parameters &quot;, lambda)))</code></pre>
<p><img src="day6_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># run ridge; alpha = 0 means do ridge
ridge.mod &lt;- glmnet(x, y, alpha = 0, lambda = grid)

# coefficient shrinkage visualized
plot(ridge.mod, xvar = &quot;lambda&quot;, label = TRUE)</code></pre>
<p><img src="day6_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code># a set of coefficients for each lambda
dim(coef(ridge.mod))</code></pre>
<pre><code>## [1]  22 100</code></pre>
<p>We can look at the coefficients at different values for <span class="math inline">\(\lambda\)</span>. Here, we randomly choose two different values and notice that smaller values of <span class="math inline">\(\lambda\)</span> result in larger coefficient estimates and vice-versa.</p>
<pre class="r"><code># Lambda and Betas
ridge.mod$lambda[80]</code></pre>
<pre><code>## [1] 0.1629751</code></pre>
<pre class="r"><code>coef(ridge.mod)[, 80]</code></pre>
<pre><code>##            (Intercept)               RSexMale             RSexFemale 
##           -0.061607390           -0.160606935            0.159900421 
##                   RAge               Househld                    Lab 
##           -0.051728204            0.082363992           -0.138922683 
##                    SNP                   Ukip                    BNP 
##            0.172199820           -0.206749980            0.425986910 
##                     GP            party.other                  paper 
##           -0.176955926            0.008250000            0.088331267 
##             WWWhourspW              religious           employMonths 
##           -0.012922053            0.010790919           -0.001149770 
##        urbanmore rural        urbanmore urban             urbanurban 
##           -0.009100653            0.049660346            0.119249438 
##        health.goodfair health.goodfairly good        health.goodgood 
##           -0.051779228           -0.006372962            0.002867504 
##                  HHInc 
##           -0.291172989</code></pre>
<pre class="r"><code>sqrt( sum(coef(ridge.mod)[-1, 80]^2) )</code></pre>
<pre><code>## [1] 0.6911836</code></pre>
<pre class="r"><code>ridge.mod$lambda[40]</code></pre>
<pre><code>## [1] 43.28761</code></pre>
<pre class="r"><code>coef(ridge.mod)[, 40]</code></pre>
<pre><code>##            (Intercept)               RSexMale             RSexFemale 
##          -0.0024035442          -0.0086089993           0.0086089995 
##                   RAge               Househld                    Lab 
##          -0.0009024882           0.0009302020          -0.0016851561 
##                    SNP                   Ukip                    BNP 
##           0.0051999098          -0.0051751362           0.0145257558 
##                     GP            party.other                  paper 
##          -0.0045065860           0.0018619440           0.0002790212 
##             WWWhourspW              religious           employMonths 
##          -0.0005069474           0.0015920318          -0.0017699007 
##        urbanmore rural        urbanmore urban             urbanurban 
##          -0.0014946533           0.0006218758           0.0040369054 
##        health.goodfair health.goodfairly good        health.goodgood 
##           0.0002138639           0.0003043203          -0.0019519248 
##                  HHInc 
##          -0.0072791705</code></pre>
<pre class="r"><code>sqrt(sum(coef(ridge.mod)[-1, 40]^2))</code></pre>
<pre><code>## [1] 0.02287352</code></pre>
<p>We can get ridge regression coefficients for any value of <span class="math inline">\(\lambda\)</span> using predict.</p>
<pre class="r"><code># compute coefficients at lambda = s
predict(ridge.mod, s = 50, type = &quot;coefficients&quot;)[1:22, ]</code></pre>
<pre><code>##            (Intercept)               RSexMale             RSexFemale 
##          -0.0020916615          -0.0075062040           0.0075062041 
##                   RAge               Househld                    Lab 
##          -0.0007828438           0.0008050887          -0.0014604733 
##                    SNP                   Ukip                    BNP 
##           0.0045109868          -0.0045029585           0.0126229971 
##                     GP            party.other                  paper 
##          -0.0039161618           0.0016221081           0.0002331180 
##             WWWhourspW              religious           employMonths 
##          -0.0004418724           0.0013894830          -0.0015446673 
##        urbanmore rural        urbanmore urban             urbanurban 
##          -0.0013036526           0.0005397837           0.0035149833 
##        health.goodfair health.goodfairly good        health.goodgood 
##           0.0001920935           0.0002676711          -0.0017039940 
##                  HHInc 
##          -0.0063276702</code></pre>
<p>Next, we can use cross-validation on ridge regression by first splitting the dataset into training and test subsets.</p>
<pre class="r"><code># cross-validate lambda by splitting dataset into training and test
set.seed(1)
train &lt;- sample(1:nrow(x), nrow(x) * .5)
y.test &lt;- y[-train]</code></pre>
<p>We estimate the parameters with <code>glmnet()</code> over the training set and predict the values on the test set to calculate the validation error.</p>
<pre class="r"><code># fit on training set
ridge.m &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)

# predict with lambda = 4
ridge.p &lt;- predict(ridge.m, s = 4, newx = x[-train, ])

# MSE on test data
mean( (ridge.p - y.test)^2 )</code></pre>
<pre><code>## [1] 0.9930511</code></pre>
<pre class="r"><code># maximal error?
mean( (mean(y[train]) - y.test)^2)</code></pre>
<pre><code>## [1] 1.057127</code></pre>
<p>In the previous example, we used <span class="math inline">\(\lambda=4\)</span> when evaluating the model on the test set. We can use a large value for <span class="math inline">\(\lambda\)</span> and see the difference in mean error.</p>
<pre class="r"><code># try for large lambda
ridge.p2 &lt;- predict(ridge.m, s = 1e+4, newx = x[-train, ])
mean((ridge.p2 - y.test)^2)</code></pre>
<pre><code>## [1] 1.057087</code></pre>
<p>We can also compare the results with a least squares model where <span class="math inline">\(\lambda = 0\)</span>.</p>
<pre class="r"><code># compare to standard logistic regression where lambda is 0
ridge.p &lt;- predict(ridge.m, s = 0, newx = x[-train, ], exact = TRUE)
mean( (ridge.p - y.test)^2 )</code></pre>
<pre><code>## [1] 0.8971826</code></pre>
<pre class="r"><code># standard lm
lm.m &lt;- lm(IMMBRIT ~ . -over.estimate -Cons, data = data2, subset = train)
lm.p &lt;- predict(lm.m, newdata = data2[-train,])
mean( (lm.p - y.test)^2 )</code></pre>
<pre><code>## [1] 0.897183</code></pre>
<p>We can choose different values for <span class="math inline">\(\lambda\)</span> by running cross-validation on ridge regression using <code>cv.glmnet()</code>.</p>
<pre class="r"><code>set.seed(1)
# training data for CV to find optimal lambda, but then test data to estimate test error
cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)</code></pre>
<p><img src="day6_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># best performing model&#39;s lambda value
bestlam &lt;- cv.out$lambda.min
bestlam</code></pre>
<pre><code>## [1] 0.242911</code></pre>
<p>The best performing model is the one with <span class="math inline">\(\lambda = 0.242911\)</span></p>
<pre class="r"><code># predict outcomes using best cross-validated lambda
ridge.pred &lt;- predict(ridge.mod, s = bestlam, newx = x[-train, ])
mean((ridge.pred - y.test)^2)</code></pre>
<pre><code>## [1] 0.8635175</code></pre>
<p>Finally, we run ridge regression on the full dataset and examine the coefficients for the model with the best MSE.</p>
<pre class="r"><code># ridge on full data
out &lt;- glmnet(x, y, alpha = 0)
predict(out, type = &quot;coefficients&quot;, s = bestlam)[1:22, ]</code></pre>
<pre><code>##            (Intercept)               RSexMale             RSexFemale 
##           -0.059777684           -0.155930812            0.155434688 
##                   RAge               Househld                    Lab 
##           -0.047003725            0.073306596           -0.120459848 
##                    SNP                   Ukip                    BNP 
##            0.170611957           -0.185986478            0.414912714 
##                     GP            party.other                  paper 
##           -0.161476647            0.015342191            0.079032354 
##             WWWhourspW              religious           employMonths 
##           -0.011987748            0.013135994           -0.005108616 
##        urbanmore rural        urbanmore urban             urbanurban 
##           -0.012686943            0.042447233            0.109851218 
##        health.goodfair health.goodfairly good        health.goodgood 
##           -0.046643963           -0.006836848           -0.002528571 
##                  HHInc 
##           -0.268558892</code></pre>
</div>
<div id="the-lasso" class="section level4">
<h4>The Lasso</h4>
<p>The lasso model can be estimated in the same way as ridge regression. The <code>alpha = 1</code> parameter tells <code>glmnet()</code> to run lasso regression instead of ridge regression.</p>
<pre class="r"><code>lasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)</code></pre>
<p><img src="day6_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Similarly, we can perform cross-validation using identical step as we did in the last exercise on ridge regression.</p>
<pre class="r"><code># cross-validation to pick lambda
set.seed(1)
cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)</code></pre>
<p><img src="day6_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>bestlam &lt;- cv.out$lambda.min
lasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[-train, ])
mean((lasso.pred - y.test)^2)</code></pre>
<pre><code>## [1] 0.8895347</code></pre>
<p>We can compare these results with ridge regression by examining the coefficient estimates.</p>
<pre class="r"><code># compare to ridge regression
out &lt;- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef &lt;- predict(out, type = &quot;coefficients&quot;, s = bestlam)[1:20, ]
lasso.coef</code></pre>
<pre><code>##            (Intercept)               RSexMale             RSexFemale 
##            0.124222196           -0.264485052            0.000000000 
##                   RAge               Househld                    Lab 
##           -0.029577571            0.066394208           -0.070239412 
##                    SNP                   Ukip                    BNP 
##            0.000000000           -0.029244738            0.261564585 
##                     GP            party.other                  paper 
##            0.000000000            0.000000000            0.006077127 
##             WWWhourspW              religious           employMonths 
##            0.000000000            0.000000000            0.000000000 
##        urbanmore rural        urbanmore urban             urbanurban 
##            0.000000000            0.000000000            0.021193805 
##        health.goodfair health.goodfairly good 
##            0.000000000            0.000000000</code></pre>
<pre class="r"><code>lasso.coef[lasso.coef != 0]</code></pre>
<pre><code>##  (Intercept)     RSexMale         RAge     Househld          Lab 
##  0.124222196 -0.264485052 -0.029577571  0.066394208 -0.070239412 
##         Ukip          BNP        paper   urbanurban 
## -0.029244738  0.261564585  0.006077127  0.021193805</code></pre>
</div>
<div id="exercises" class="section level4">
<h4>Exercises</h4>
<div id="q1" class="section level5">
<h5>Q1</h5>
<p>In this exercise, we will predict the number of applications received using the <code>College</code> data set. You need to load <code>libary(ISLR)</code> and then type <code>?College</code> to get the codebook.</p>
<ol style="list-style-type: decimal">
<li>Split the data set into a training set and a test set.</li>
<li>Fit a linear model using least squares on the training set, and report the test error obtained.</li>
<li>Fit a ridge regression model on the training set, with <span class="math inline">\(\lambda\)</span> chosen by cross-validation. Report the test error obtained.</li>
<li>Fit a lasso model on the training set, with <span class="math inline">\(\lambda\)</span> chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.</li>
<li>Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?</li>
</ol>
</div>
<div id="q2" class="section level5">
<h5>Q2</h5>
<p>We will now try to predict the per capita crime rate in the <code>Boston</code> data set. The <code>Boston</code> data set is in the <code>MASS</code> library.</p>
<ol style="list-style-type: decimal">
<li>Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, and ridge regression. Present and discuss results for the approaches that you consider.</li>
<li>Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error.</li>
<li>Does your chosen model involve all of the features in the data set? Why or why not?</li>
</ol>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
