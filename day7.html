<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Philipp Broniecki and Lucas Leemann – Machine Learning 1K" />


<title>Lab 7 – Polynomial Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/sandstone.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Essex 2017 Machine Learning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 1
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day1.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D1%20-%20Intro%20ML.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions1.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 2
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day2.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D2%20-%20Intro%20ML.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions2.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 3
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day3.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D3%20-%20Classification.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions3.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 4
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day4.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D4%20-%20Resampling.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions4.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 5
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day5.html">Lab</a>
    </li>
    <li>
      <a href="labs/Lab%20Code%205.R">plain R-Code</a>
    </li>
    <li>
      <a href="./slides/D5%20-%20Model%20Selection%20I.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions5.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 6
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day6.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D6%20-%20Model%20Selection%20II.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions6.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 7
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day7.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D7%20-%20Polynomial%20Models.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions7.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 8
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day8.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D8%20-%20Tree-Based%20Methods.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions8.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Day 9
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="day9.html">Lab</a>
    </li>
    <li>
      <a href="./slides/D9%20-%20Unsupervised%20Learning.pdf">Slides</a>
    </li>
    <li>
      <a href="solutions9.html">Solutions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="simulation.html">Simulation</a>
    </li>
    <li>
      <a href="montecarlo.html">Monte Carlos</a>
    </li>
    <li>
      <a href="MCLassoRidge.html">MC Lasso v. Ridge</a>
    </li>
    <li>
      <a href="splinesCV.html">Splines Cross Validated</a>
    </li>
    <li>
      <a href="./data/titanic.dta">Titanic Data</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Lab 7 – Polynomial Models</h1>
<h4 class="author"><em>Philipp Broniecki and Lucas Leemann – Machine Learning 1K</em></h4>

</div>


<div id="based-on-james-et-al.-2013" class="section level5">
<h5>(based on James et al. 2013)</h5>
<p>We begin by loading that <code>ISLR</code> package and attaching to the Wage dataset that we will be using throughout this exercise.</p>
<pre class="r"><code># clear workspace, load ISLR, attach wage data set
rm(list=ls())
library(ISLR)
attach(Wage)
?Wage # codebook</code></pre>
</div>
<div id="polynomial-regression" class="section level3">
<h3>Polynomial Regression</h3>
<p>Let’s fit a linear model to predict <code>wage</code> with a forth-degree polynomial using the <code>poly()</code> function.</p>
<pre class="r"><code># linear regression on wage, with age up to a 4th degree polynomial
fit &lt;- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(fit))</code></pre>
<pre><code>##                 Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)    111.70361  0.7287409 153.283015 0.000000e+00
## poly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28
## poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32
## poly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03
## poly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02</code></pre>
<p>We can also obtain raw instead of orthogonal polynomials by passing the <code>raw = TRUE</code> argument to <code>poly()</code>. The coefficients will change the fit should be largely unaffected.</p>
<pre class="r"><code>fit2 &lt;- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)
coef(summary(fit2))</code></pre>
<pre><code>##                                Estimate   Std. Error   t value
## (Intercept)               -1.841542e+02 6.004038e+01 -3.067172
## poly(age, 4, raw = TRUE)1  2.124552e+01 5.886748e+00  3.609042
## poly(age, 4, raw = TRUE)2 -5.638593e-01 2.061083e-01 -2.735743
## poly(age, 4, raw = TRUE)3  6.810688e-03 3.065931e-03  2.221409
## poly(age, 4, raw = TRUE)4 -3.203830e-05 1.641359e-05 -1.951938
##                               Pr(&gt;|t|)
## (Intercept)               0.0021802539
## poly(age, 4, raw = TRUE)1 0.0003123618
## poly(age, 4, raw = TRUE)2 0.0062606446
## poly(age, 4, raw = TRUE)3 0.0263977518
## poly(age, 4, raw = TRUE)4 0.0510386498</code></pre>
<p>There are several ways to specify polynomials. These are, however a little less convenient.</p>
<pre class="r"><code>fit2a &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)</code></pre>
<pre><code>##   (Intercept)           age      I(age^2)      I(age^3)      I(age^4) 
## -1.841542e+02  2.124552e+01 -5.638593e-01  6.810688e-03 -3.203830e-05</code></pre>
<p>A more compact version of the same example uses <code>cbind()</code> and eliminates the need to wrap each term in <code>I()</code>. The output is less readable though.</p>
<pre class="r"><code>fit2b &lt;- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage)
coef(fit2b)</code></pre>
<pre><code>##                        (Intercept) cbind(age, age^2, age^3, age^4)age 
##                      -1.841542e+02                       2.124552e+01 
##    cbind(age, age^2, age^3, age^4)    cbind(age, age^2, age^3, age^4) 
##                      -5.638593e-01                       6.810688e-03 
##    cbind(age, age^2, age^3, age^4) 
##                      -3.203830e-05</code></pre>
<p>We can create an age grid for the targeted values of the prediction and pass the grid to <code>predict()</code>. We can set the argument <code>se=TRUE</code> in the <code>predict()</code> function which will return a list that includes standard errors of the outcome. We can use these to an upper and lower bound of our estimate of <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code># minimum and maximum values of age variable
agelims &lt;- range(age) 
age.grid &lt;- seq(from = agelims[1], to = agelims[2])
# se=TRUE returns standard errors
preds &lt;- predict(fit, newdata = list(age = age.grid), se = TRUE)
# confidence intervals as estimate + and - 2 standard deviations
se.bands &lt;- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)</code></pre>
<p>We can plot the data and add the fit from the degree-4 polynomial. We set the margins and outer margins in our plot the later plot a title that will be the overall title for two plots that we plot next to each other. The function <code>matlines()</code> lets us draw the lines fo the uncertainty bounds in one go.</p>
<pre class="r"><code># set margins to plot title in margins
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(wage ~ jitter(age,2), xlim = agelims, cex = 0.5, col = &quot;darkgrey&quot;, bty = &quot;n&quot;,
     xlab = &quot;age&quot;)
# overall plot window title
title(&quot;Degree -4 Polynomial &quot;, outer = TRUE)
# line for mean estimate
lines(age.grid, preds$fit, lwd = 2, col = &quot;blue&quot;)
# ~95% ci&#39;s
matlines(age.grid, se.bands, lwd = 2, col = &quot;blue&quot;, lty = 3)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>We can compare the orthogonolized polynomials that we saved in the object called <code>fit</code> with the polynomials that plain polynomials sved in <code>fit2</code>. The difference will substantially <span class="math inline">\(0\)</span>. We predict the outcome from the fit with the raw polynomials and take the diffence to the fit with the independent linear combinations of the powers of age.</p>
<pre class="r"><code>preds2 &lt;- predict(fit2, newdata = list(age = age.grid), se = TRUE)
# average difference
mean(preds$fit - preds2$fit)</code></pre>
<pre><code>## [1] -1.752311e-11</code></pre>
<pre class="r"><code># maximum difference
max(abs(preds$fit - preds2$fit))</code></pre>
<pre><code>## [1] 7.81597e-11</code></pre>
<p>When we have only predictor variable and and its powers we use the <code>coef()</code> function to see whether the powers of the variable improve in-sample model fit.</p>
<pre class="r"><code>fit.5 &lt;- lm(wage ~ poly(age, 5), data = Wage)
coef(summary(fit.5))</code></pre>
<pre><code>##                 Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)    111.70361  0.7287647 153.2780243 0.000000e+00
## poly(age, 5)1  447.06785 39.9160847  11.2001930 1.491111e-28
## poly(age, 5)2 -478.31581 39.9160847 -11.9830341 2.367734e-32
## poly(age, 5)3  125.52169 39.9160847   3.1446392 1.679213e-03
## poly(age, 5)4  -77.91118 39.9160847  -1.9518743 5.104623e-02
## poly(age, 5)5  -35.81289 39.9160847  -0.8972045 3.696820e-01</code></pre>
<p>Preferably, we use the <code>anova()</code> and look at the F-test to decide whether in-sample fit improves by including powers of a variable.</p>
<pre class="r"><code>fit.1 &lt;- lm(wage ~ age, data = Wage)
fit.2 &lt;- lm(wage ~ poly(age, 2), data = Wage)
fit.3 &lt;- lm(wage ~ poly(age, 3), data = Wage)
fit.4 &lt;- lm(wage ~ poly(age, 4), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: wage ~ age
## Model 2: wage ~ poly(age, 2)
## Model 3: wage ~ poly(age, 3)
## Model 4: wage ~ poly(age, 4)
## Model 5: wage ~ poly(age, 5)
##   Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    
## 1   2998 5022216                                    
## 2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***
## 3   2996 4777674  1     15756   9.8888  0.001679 ** 
## 4   2995 4771604  1      6070   3.8098  0.051046 .  
## 5   2994 4770322  1      1283   0.8050  0.369682    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We prefer the F-test beacuse it works when we include additional predictors into our model.</p>
<pre class="r"><code># comparing models when other predictors are added
fit.1 &lt;- lm(wage ~ education + age, data = Wage)
fit.2 &lt;- lm(wage ~ education + poly(age, 2), data = Wage)
fit.3 &lt;- lm(wage ~ education + poly(age, 3), data = Wage)
anova(fit.1, fit.2, fit.3)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: wage ~ education + age
## Model 2: wage ~ education + poly(age, 2)
## Model 3: wage ~ education + poly(age, 3)
##   Res.Df     RSS Df Sum of Sq        F Pr(&gt;F)    
## 1   2994 3867992                                 
## 2   2993 3725395  1    142597 114.6969 &lt;2e-16 ***
## 3   2992 3719809  1      5587   4.4936 0.0341 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>With <code>glm()</code> we can also fit a polynomial logistic regression. Here, we create a binary variable that is 1 if wage &gt; 250 and 0 otherwise.</p>
<pre class="r"><code>fit &lt;- glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = binomial)</code></pre>
<p>Similar to <code>lm()</code> we use the <code>predict()</code> function again and also obtain standard errors by setting <code>se=TRUE</code>.</p>
<p>Note: If we do <strong>not</strong> set <code>type=&quot;response&quot;</code> in the <code>predict()</code> function, we get the latent <span class="math inline">\(y\)</span> as <span class="math inline">\(X\beta\)</span>. We have to send those values through the link function to get predicted probabilities. We do this, so that we can estimate the standard errors on the latent <span class="math inline">\(y\)</span>. We then send these through the link function as well. This ensures that our confidence intervals will never be outside the logical <span class="math inline">\([0, 1]\)</span> interval for probabilities.</p>
<pre class="r"><code># predict latent y
preds &lt;- predict(fit, newdata = list(age = age.grid), se = TRUE)

# send latent y through the link function
pfit &lt;- 1 / (1 + exp(-preds$fit))

# error bands calculate on the latent y
se.bands.logit &lt;- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands &lt;- 1 / (1 + exp(-se.bands.logit))</code></pre>
<p>We add the results next to the plot where wage is continuous. With the <code>points()</code> function we add the actual data to the plot. The argument <code>pch=&quot;|&quot;</code> draws a bar as the symbol for each point. Also notice the y-coordinate of each point. In the <code>plot()</code> function we set the range of the y-axis with <code>ylim = c(0, 0.2)</code> to range from <span class="math inline">\(0\)</span> to <span class="math inline">\(0.2\)</span>. If the true outcome is <span class="math inline">\(1\)</span> we want to draw the | at <span class="math inline">\(y=0.2\)</span> and otherwhise at <span class="math inline">\(y=0\)</span>. We achieve this with <code>I((wage &gt; 250)/5)</code>. Play around to see why.</p>
<pre class="r"><code>plot(I(wage &gt; 250) ~ age, xlim = agelims, type = &quot;n&quot;, ylim = c(0, 0.2))
# add data to the plot
points(jitter(age), I((wage &gt; 250)/5) , cex = 1, pch = &quot;|&quot;, col = &quot; darkgrey &quot;)
# mean estimate
lines(age.grid, pfit, lwd = 2, col = &quot;blue&quot;)
# 95 ci
matlines(age.grid, se.bands, lwd = 2, col = &quot;blue&quot;, lty = 3)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Notice, that the confidence interval becomes very large in the range of the data where we have few data and no <span class="math inline">\(1\)</span>’s.</p>
</div>
<div id="step-functions" class="section level3">
<h3>Step Functions</h3>
<p>Instead of using polynomials to create a non-linear prediction, we could also use step functions. With step functions we fit different lines for different data ranges.</p>
<p>We use the <code>cut()</code> function to create equally spaced cutpoints in our data. We use the now categorical variable age as predictor in our linear model.</p>
<pre class="r"><code># four equally spaced intervals of age
table(cut(age, 4))</code></pre>
<pre><code>## 
## (17.9,33.5]   (33.5,49]   (49,64.5] (64.5,80.1] 
##         750        1399         779          72</code></pre>
<pre class="r"><code># fit the linear regression with the factor variable age that has four categories
fit &lt;- lm(wage ~ cut(age, 4), data = Wage)
# the first category is the baseline.
coef(summary(fit))</code></pre>
<pre><code>##                         Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)            94.158392   1.476069 63.789970 0.000000e+00
## cut(age, 4)(33.5,49]   24.053491   1.829431 13.148074 1.982315e-38
## cut(age, 4)(49,64.5]   23.664559   2.067958 11.443444 1.040750e-29
## cut(age, 4)(64.5,80.1]  7.640592   4.987424  1.531972 1.256350e-01</code></pre>
</div>
<div id="splines" class="section level3">
<h3>Splines</h3>
<p>We use the <code>splines</code> package to fit splines.</p>
<pre class="r"><code>library(splines)</code></pre>
<p>We first use <code>bs()</code> to generate a basis matrix for a polynomial spline and fit a model with knots at age 25, 40 and 60. <code>bs</code> will by default fit a cubic spline with the specified number of knots. To deviate from a cubic spline, chanbe the argument <code>degree</code> to some other value.</p>
<pre class="r"><code>fit &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred &lt;- predict(fit, newdata = list(age = age.grid), se = TRUE)
par( mfrow = c(1,1))
plot(jitter(age,2), wage, col = &quot;gray&quot;, xlab = &quot;age&quot;, bty = &quot;n&quot;)
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = &quot;dashed&quot;, lwd = 2)
lines(age.grid, pred$fit - 2 * pred$se, lty = &quot;dashed&quot;, lwd = 2)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The confidence intervals are wide at the end of data range whree the curve is forced off into some direction by only a few observations.</p>
<p>Instead of specifying knots ourselves we can set the argument <code>df</code> (degrees of freedom) to some value to set knots at uniform intervals. A polynomial of the power of three uses up four degrees of freedom. One for each power. Where we to fit polynomials in regions of the data we woud use 4 degrees of freedom in each region. Fitting a cubic spline involves three predictors and an intercept = 4. In addition we use a degree of freedom at each knot.</p>
<pre class="r"><code>dim( bs(age, knots = c(25, 40, 60)) )</code></pre>
<pre><code>## [1] 3000    6</code></pre>
<pre class="r"><code>dim(bs(age, df = 6))</code></pre>
<pre><code>## [1] 3000    6</code></pre>
<pre class="r"><code>attr(bs(age, df = 6), &quot;knots&quot;)</code></pre>
<pre><code>##   25%   50%   75% 
## 33.75 42.00 51.00</code></pre>
<p>To address the problem of the wide error bands at the ends of the data range, we can fit natural splines. They impose linearity at the end of the range, are better behaved (produce narror error bands) and free up two degrees of freedom. One at each end. We <code>ns()</code> for natural splines.</p>
<pre class="r"><code>plot(jitter(age,2), wage, col = &quot;gray&quot;, bty = &quot;n&quot;, xlab = &quot;age&quot;)
fit2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 &lt;- predict(fit2, newdata = list(age = age.grid), se = TRUE)
lines(age.grid, pred2$fit, col = &quot;red&quot;, lwd = 2)
lines(age.grid, pred2$fit + 2 * pred$se, lty = &quot;dashed&quot;, lwd = 2, col = &quot;red&quot;)
lines(age.grid, pred2$fit - 2 * pred$se, lty = &quot;dashed&quot;, lwd = 2, col = &quot;red&quot;)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<div id="local-regression" class="section level3">
<h3>Local Regression</h3>
<p>One can also account for non-linearity by using local regression. We estimate the regression curve only in the near neighborhood of the the current point. Smoothness is achieved by moving along the data range gradually whereby the intervals overlap. Like k-nearest neighbors local regression can does not work well in high dimensions (many predictors) because the distances between points become very large.</p>
<p>Here, wer use the <code>loess()</code> smoother for local regression. The argument <code>span</code> specifies the range of the data that is being used. The narrower the span the more flexible and wiggly the line will become.</p>
<pre class="r"><code>plot(jitter(age,2), wage, xlim = agelims, cex = 0.5, col = &quot;darkgrey&quot;, bty = &quot;n&quot;, xlab = &quot;age&quot;)
title(&quot; Local Regression &quot;)
fit &lt;- loess(wage ~ age, span = 0.2, data = Wage)
fit2 &lt;- loess(wage ~ age, span = 0.5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)), col = &quot;red&quot;, lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = &quot;blue&quot;, lwd = 2)
legend(&quot;topright&quot;, legend = c(&quot;Span=0.2&quot;, &quot;Span=0.5&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), 
       lty = 1, lwd = 2, cex = 0.8, bty = &quot;n&quot;)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="genearlized-additive-models-gams" class="section level3">
<h3>Genearlized Additive Models (GAMs)</h3>
<p>With GAMs we can flexibly combine the mehtods we introduced above for multible predictors.</p>
<p>For splines, we can fit a GAM with <code>lm()</code> when an appropriate basis functions can be used.</p>
<pre class="r"><code>gam1 &lt;- lm(wage ~ bs(year, 4) + bs(age, 5) + education, data = Wage)</code></pre>
<p>However, the <code>gam</code> package offers a general solution to fitting GAMs and is especially useful when splines cannot be easily expressed in terms of basis functions. Here, we fit natural splines on age and year. Something that could not easily be expressed as a basis function.</p>
<pre class="r"><code>library(gam)
# gam with natural splines for year and age
gam.m3 &lt;- gam(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)</code></pre>
<p>We can use the <code>plot()</code> function as well as <code>plot.gam()</code> to plot our results. The difference between cubic splines and natural splines fit to age and education is very small.</p>
<pre class="r"><code>par(mfrow = c(1, 3), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
# plot function behaves similar to plot.gam
plot(gam.m3, se = TRUE, col = &quot;blue&quot;)
title(&quot;Natural splines for age and education&quot;, outer = TRUE)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 3), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot.gam(gam1, se = TRUE, col = &quot;red&quot;)
title(&quot;Cubic splines for age and education&quot;, outer = TRUE)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<p>We can use <code>anova()</code> to find the best performing model.</p>
<pre class="r"><code>gam.m1 &lt;- gam(wage ~ ns(age, 5) + education, data = Wage)
gam.m2 &lt;- gam(wage ~ year + ns(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = &quot;F&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: wage ~ ns(age, 5) + education
## Model 2: wage ~ year + ns(age, 5) + education
## Model 3: wage ~ ns(year, 4) + ns(age, 5) + education
##   Resid. Df Resid. Dev Df Deviance       F    Pr(&gt;F)    
## 1      2990    3712881                                  
## 2      2989    3694885  1  17996.1 14.5551 0.0001389 ***
## 3      2986    3691919  3   2966.4  0.7997 0.4938916    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that looking at the <code>summary()</code> function would not have been enough to determine whether a natural spline on year is appropriate. The coeffiecient is highly significant. However, the F-test reveals that the model is not significantly better than the model without a natural spline for year.</p>
<pre class="r"><code>summary(gam.m3)</code></pre>
<pre><code>## 
## Call: gam(formula = wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -120.513  -19.608   -3.583   14.112  214.535 
## 
## (Dispersion Parameter for gaussian family taken to be 1236.409)
## 
##     Null Deviance: 5222086 on 2999 degrees of freedom
## Residual Deviance: 3691919 on 2986 degrees of freedom
## AIC: 29889.5 
## 
## Number of Local Scoring Iterations: 2 
## 
## Anova for Parametric Effects
##               Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
## ns(year, 4)    4   25102    6275   5.0755 0.0004477 ***
## ns(age, 5)     5  453232   90646  73.3141 &lt; 2.2e-16 ***
## education      4 1051834  262958 212.6791 &lt; 2.2e-16 ***
## Residuals   2986 3691919    1236                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can make predictions using <code>predict()</code> just as we did before.</p>
<pre class="r"><code>preds &lt;- predict(gam.m2, newdata = Wage)</code></pre>
<p>The <code>gam()</code> function also allows fitting logistic regression GAM with the <code>family = binomial</code> argument.</p>
<pre class="r"><code>gam.lr &lt;- gam(I(wage &gt; 250) ~ year + ns(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = &quot;green&quot;)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Looking at the education variable, we see that the first category has extremely wide error bands. We check a cross-table of education against the indicator of whether age &gt; 250. We see that age &gt; 250 is never true for lowest category.</p>
<pre class="r"><code>table(education, I(wage &gt; 250))</code></pre>
<pre><code>##                     
## education            FALSE TRUE
##   1. &lt; HS Grad         268    0
##   2. HS Grad           966    5
##   3. Some College      643    7
##   4. College Grad      663   22
##   5. Advanced Degree   381   45</code></pre>
<p>Based on this we exclude the lowest education category from our model.</p>
<pre class="r"><code>gam.lr.s &lt;- gam(I(wage &gt; 250) ~ year + ns(age, df = 5) + education, family = binomial, data = Wage, subset = (education != &quot;1. &lt; HS Grad&quot;))
plot(gam.lr.s, se = TRUE, col = &quot;green&quot;)</code></pre>
<p><img src="day7_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="exercises" class="section level3">
<h3>Exercises</h3>
<div id="q1" class="section level4">
<h4>Q1</h4>
<p>In this exercise, you will further analyze the <code>Wage</code> dataset coming with the <code>ISLR</code> package.</p>
<ol style="list-style-type: decimal">
<li>Perform polynomial regression to predict <code>wage</code> using <code>age</code>. Use cross-validation to select the optimal degree dd for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using <code>ANOVA</code>? Make a plot of the resulting polynomial fit to the data.</li>
<li>Fit a step function to predict <code>wage</code> using <code>age</code>, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained.</li>
</ol>
</div>
<div id="q2" class="section level4">
<h4>Q2</h4>
<p>The <code>Wage</code> data set contains a number of other features that we haven’t yet covered, such as marital status (<code>maritl</code>), job class (<code>jobclass</code>), and others. Explore the relationships between some of these other predictors and <code>wage</code>, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings.</p>
</div>
<div id="q3" class="section level4">
<h4>Q3</h4>
<p>This question uses the variables <code>dis</code> (the weighted mean of distances to five Boston employment centers) and <code>nox</code> (nitrogen oxides concentration in parts per 10 million) from the <code>Boston</code> data available as part of the <code>MASS</code> package. We will treat <code>dis</code> as the predictor and <code>nox</code> as the response.</p>
<ol style="list-style-type: decimal">
<li>Use the <code>poly()</code> function to fit a cubic polynomial regression to predict <code>nox</code> using <code>dis</code>. Report the regression output, and plot the resulting data and polynomial fits.</li>
<li>Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.</li>
<li>Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.</li>
<li>Use the <code>bs()</code> function to fit a regression spline to predict <code>nox</code> using <code>dis</code>. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.</li>
<li>Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.</li>
</ol>
</div>
<div id="q4" class="section level4">
<h4>Q4</h4>
<p>This question relates to the <code>College</code> dataset from the <code>ISLR</code> package.</p>
<ol style="list-style-type: decimal">
<li>Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.</li>
<li>Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.</li>
<li>Evaluate the model obtained on the test set, and explain the results obtained.</li>
<li>For which variables, if any, is there evidence of a non-linear relationship with the response?</li>
</ol>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
